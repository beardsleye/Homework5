---
title: "caretPackage"
format: html
---

# Task 1
1. We cross validate to ensure accuracy when fitting the model with random forest model. There for allowing us to find the best subset of predictors.
2. First, you create a boot strap sample. Next, train the tree on this sample. Then, repeat this 10,000 times. Lastly, average across the trees to decrease variance. Then, use the OOB observations to find a prediction error.
3. A general linear model is used for a continuous response that allows for both continuous and categorical predictors.
4. Allowing an interaction term allows for relationships between variables to be accounted for in the model.
5. We split our data into the train and test set to properly fit our model. The training set is for fitting the model, where the test set is to test the performance of the fit.

# Task 2
## EDA and Data Prep
```{r EDA and data prep}
library(readr)
library(tidyverse)
library(psych)
library(caret)

#read in data and make HeartDisease a factor
heart <- read_csv("C:/Users/beard/Downloads/heart.csv")
heart_df<-as_tibble(heart) |>
  mutate( ,HeartDisease = as.factor(HeartDisease))
heart_df

#Show missing counts, find none
na_counts<- colSums(is.na(heart_df))
na_counts

#Summarize numeric variables
heart_df |>
  select(c(Age, RestingBP, Cholesterol, FastingBS, MaxHR, Oldpeak)) |>
  describe()

#Plot correlations between numeric variables
heart_df |>
  select(c(Age, RestingBP, Cholesterol, FastingBS, MaxHR, Oldpeak)) |>
  pairs()

#Following looks at contingency tables of categorical variables and Heart Disease
heart_df |>
  group_by(HeartDisease, Sex) |>
  summarize(count = n()) |>
  pivot_wider(names_from = Sex, values_from = count)

heart_df |>
  group_by(HeartDisease, ChestPainType) |>
  summarize(count = n()) |>
  pivot_wider(names_from = ChestPainType, values_from = count)

heart_df |>
  group_by(HeartDisease, RestingECG) |>
  summarize(count = n()) |>
  pivot_wider(names_from = RestingECG, values_from = count)

heart_df |>
  group_by(HeartDisease, ExerciseAngina) |>
  summarize(count = n()) |>
  pivot_wider(names_from = ExerciseAngina, values_from = count)

heart_df |>
  group_by(HeartDisease, ST_Slope) |>
  summarize(count = n()) |>
  pivot_wider(names_from = ST_Slope, values_from = count)

#Create dummy variables
dummies<-dummyVars(HeartDisease~., data = heart_df) 
dummy_vars<-predict(dummies, newdata = heart_df)
dummy_vars<-as.data.frame(dummy_vars)
dummy_vars

#Remove ST_Slope variables
dummy_vars<-dummy_vars |>
  select(!c(Age, RestingBP, Cholesterol, MaxHR, FastingBS, Oldpeak))

#Create tibble with all variables
heart_vars<-cbind(heart_df, dummy_vars)
heart_vars<-heart_vars |>
  select(!c(starts_with("ST_Slope")))
heart_vars
```
##Split data and KNN
```{r Split and KNN}
library(class)
library(randomForest)
set.seed(66)

#create train and test sets
train <- sample(1:nrow(heart_vars), size = nrow(heart_vars)*0.8)
test <- setdiff(1:nrow(heart_vars), train)

heart_train<-heart_vars[train, ]
heart_test<-heart_vars[test, ]

#do KNN
knn_fit<-train(HeartDisease~Age+RestingBP+Cholesterol+FastingBS+MaxHR+Oldpeak, data=heart_train,
               method= "knn",
               trControl=trainControl(method = "repeatedcv", number = 10, repeats = 3),
               tuneGrid=expand.grid(k=seq(from = 1, to = 40, by = 1)))

#confusion matrix
test_pred<-predict(knn_fit, newdata=heart_test)
knn_confusion<-confusionMatrix(test_pred, heart_test$HeartDisease)
knn_confusion
knn_accuarcy<-knn_confusion$overall["Accuracy"]

```
## Logistic Regression
```{r Logistic}
#Select 3 different logistic regressions using EDA as insight
glm1_fit<-train(HeartDisease~., data=heart_train[,1:11], method= "glm", family="binomial",trControl=trainControl(method = "repeatedcv", number = 10, repeats = 3))

glm2_fit<-train(HeartDisease~.+Age:MaxHR, data=heart_train[,1:11], method= "glm", family="binomial",trControl=trainControl(method = "repeatedcv", number = 10, repeats = 3))

glm3_fit<-train(HeartDisease~.+Sex:Cholesterol, data=heart_train[,1:11], method= "glm", family="binomial",trControl=trainControl(method = "repeatedcv", number = 10, repeats = 3))

summary_glm1<-summary(glm1_fit)
summary_glm2<-summary(glm2_fit)
summary_glm3<-summary(glm3_fit)

#create table of results to find best model
results<-rbind(c("glm1", summary_glm1$aic),
               c("glm2", summary_glm2$aic),
               c("glm3", summary_glm3$aic))

results

#print summary of best model (model 1 has lowest aic)
summary_glm1

#confusion matrix for best model
test_pred_glm<-predict(glm1_fit, newdata=heart_test)
best_glm_confusion<-confusionMatrix(test_pred_glm, heart_test$HeartDisease)
best_glm_confusion
glm_accuracy<-best_glm_confusion$overall["Accuracy"]
```
## Tree Models
```{r Tree Models}
library(randomForest)
library("gbm")
#classification trees
class_tree<-train(HeartDisease~., data=heart_train[,1:11], method="rpart",
                  trControl=trainControl(method = "cv", number = 10),
                  tuneGrid=expand.grid(cp = seq(0, 0.1, by = 0.001)))

#random forest
random_forest<-train(HeartDisease~., data=heart_train[,1:11], method="rf",
                  trControl=trainControl(method = "cv", number = 10),
                  tuneGrid=expand.grid(mtry =c(1:10)))

#boosted tree
boost_tree<-train(HeartDisease~., data=heart_train[,1:11], method="gbm",
trControl=trainControl(method = "cv", number = 10),
                  tuneGrid=expand.grid(
                  n.trees= c(25,50,100,200),
                  interaction.depth=c(1,2,3),
                  shrinkage=.1,
                  n.minobsinnode=10), verbose=FALSE)

#classification trees confusion matrix
test_pred_class<-predict(class_tree, newdata=heart_test)
class_confusion<-confusionMatrix(test_pred_class, heart_test$HeartDisease)
class_confusion
class_accuracy<-class_confusion$overall["Accuracy"]

#random forest confusion matrix
test_pred_rf<-predict(random_forest, newdata=heart_test)
rf_confusion<-confusionMatrix(test_pred_rf, heart_test$HeartDisease)
rf_confusion
rf_accuracy<-rf_confusion$overall["Accuracy"]

#boosted trees confusion matrix
test_pred_bt<-predict(boost_tree, newdata=heart_test)
bt_confusion<-confusionMatrix(test_pred_bt, heart_test$HeartDisease)
bt_confusion
bt_accuracy<-bt_confusion$overall["Accuracy"]
```

# Wrap
Looking at each methods accuracy to see best model on test set.
```{r Accuracy}
accuracy<-c("KNN"=knn_accuarcy,"Logistic"=glm_accuracy,"Classification Tree"=class_accuracy,"Random Forest"=rf_accuracy, "Boosted Tree"=bt_accuracy)
accuracy
```
Therefor Random Forest did the best as it had the highest accuracy.
